{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYLi5n891h9N/QImgbVAtF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraKmair/Latent-Dirichlet-Allocation-LDA/blob/main/LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVN8mndBnx6C"
      },
      "outputs": [],
      "source": [
        "#import the libraries \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA \n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.svm import SVC #support vector classifier\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from matplotlib import pyplot as plt \n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read the df\n",
        "df = pd.read_csv('')"
      ],
      "metadata": {
        "id": "6cGca8_qp0Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling the df\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df)"
      ],
      "metadata": {
        "id": "b9XTqRahqXL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the class mean \n",
        "np.set_printoptions(precision=1) #number of decimal \n",
        "mean_vectors = []\n",
        "for i in range(): #range should be here the corresponding label ie. range(3,6) labels: 3, 4, 5\n",
        "  mean_vectors.append(np.mean(scaled_df[label == i], axis = 0))\n"
      ],
      "metadata": {
        "id": "Sx94u9EtqgYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating the within and between class scatter metrix \n",
        "s_w = np.zeros((featuresNumber,featuresNumber)) #create a matrix # of features x # of features\n",
        "for i, mv in zip(range(), mean_vectors): #range should be the corresponding labels \n",
        "  class_sc_mat = np.zeros(()) # number of features \n",
        "  for row in scaled_df[label==i]:\n",
        "    row, mv = row.reshape(11,1), mv.reshape(11,1)\n",
        "    class_sc_mat+= (row-mv).dot((row-mv).T)\n",
        "  s_w+= class_sc_mat"
      ],
      "metadata": {
        "id": "MRd-Hqp0rynR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the betwee-class scatter matrix \n",
        "overall_mean = np.mean(scaled_df, axis = 0)\n",
        "c = 0\n",
        "s_b = np.zeros((11,11)) #number of features\n",
        "for i, mean_vec in enumerate(mean_vectors):\n",
        "  n = scaled_df[label ==i+3, :].shape[0] #i here is the label number\n",
        "  mean_vec= mean_vec.reshape(11,1)\n",
        "  overall_mean = overall_mean.reshape(11,1)\n",
        "  s_b+=n*(mean_vec - overall_mean).dot((mean_vec-overall_mean).T)"
      ],
      "metadata": {
        "id": "tVphnTaptdGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the eigenvectors and eigenvalues pairs\n",
        "#eigenvalues to find the percentege of variance carried by each linear discriminants (eigenvectors)\n",
        "eig_vals, eig_vecs = np.linalg.eeig(np.linalg.inv(s_w).dot(s_b))"
      ],
      "metadata": {
        "id": "R6d2cWYpuv_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape the matrix\n",
        "for i in range((len(eig_vals))):\n",
        "  eig_vec_sc = eig_vecs[:, i].reshape(11,1) #number of features \n"
      ],
      "metadata": {
        "id": "p2eBvjRGvIgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the variance %\n",
        "eig_pairs = []\n",
        "for i in range(len(eig_vals)):\n",
        "  if eig_vals[i]<0:\n",
        "    eig_pairs.append((-eig_vals[i], -eig_vecs[:,i]))\n",
        "    else:\n",
        "      eig_pairs.append((eig_vals[i], eig_vecs[:,i]))\n",
        "eig_pairs = sorted(eig_pairs, key = lambda k:k[0], reverse = True)"
      ],
      "metadata": {
        "id": "qJVE2QSsv0xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the explained variance \n",
        "print('Explained Variance:\\n ')\n",
        "eigv_sum = sum(eig_vals)\n",
        "for i, j in enumerate(eig_pairs):\n",
        "  print('eigenvalue'.format(i+1, (j[0].real/eigv_sum.real)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4U1wG4tq7q5",
        "outputId": "36284c24-243e-4bff-f7a0-5aa60f6297b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance:\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#take the first x eigenvectors based on the variance explained > 80%\n",
        "w = np.hstack(eig_pairs[0][1].reshape(11,1), eig_pairs[1][1].reshape(11,1))\n",
        "#apply lda on the original data (dimensionality reduction)\n",
        "df_lda = scaled_df.dot(w)"
      ],
      "metadata": {
        "id": "tEKTrdgvrFwI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}